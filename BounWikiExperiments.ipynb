{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**BounWiki - Experiments**\n",
        "\n",
        "- Tutorial for running Experiments from the report and evaluate them"
      ],
      "metadata": {
        "id": "a34LjFJP0gku"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nW6aMGPxn-X"
      },
      "source": [
        "##**Installs and Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLIrY8_qxn-Z",
        "outputId": "70ca504b-ba92-474c-c997-ef67005f44f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/mnt\n",
            "/content/mnt/MyDrive/Erasmus/Transformer_Pretraining/Bounwiki\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/mnt')\n",
        "%cd ./mnt/MyDrive/Erasmus/Transformer_Pretraining/Bounwiki"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html"
      ],
      "metadata": {
        "id": "Lep39vxA4sZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]"
      ],
      "metadata": {
        "id": "7ekICcj9dF86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## RESTART KERNEL and continue with cells below"
      ],
      "metadata": {
        "id": "kMbgDzFE2ebY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-fnHVdXGqy9"
      },
      "outputs": [],
      "source": [
        "# check if all imports work correctly\n",
        "import time\n",
        "import logging\n",
        "import os\n",
        "import torch\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "from haystack.nodes.retriever import EmbeddingRetriever\n",
        "from haystack.nodes import TableReader, FARMReader, RouteDocuments, JoinAnswers\n",
        "from haystack.utils import print_answers\n",
        "from haystack import Document\n",
        "from haystack import Pipeline\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import torch_scatter\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "FIgMrs8Bz1rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnmpFC1qxn-d"
      },
      "source": [
        "##**Run experiments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy4e463s89ci"
      },
      "outputs": [],
      "source": [
        "## if desired, instead of default in-memory document store, you can use elasticsearch. \n",
        "## Please change line 11 in \"setup_database.py\" to ElasticSearchDocumentStore() if you do so\n",
        "\n",
        "## download elasticsearch\n",
        "# %%bash\n",
        "# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "# tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "# chown -R daemon:daemon elasticsearch-7.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBUNJRnV9Cc8"
      },
      "outputs": [],
      "source": [
        "## start elasticsearch server\n",
        "# %%bash --bg\n",
        "# sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## wait until elastich search server started\n",
        "#time.sleep(30)"
      ],
      "metadata": {
        "id": "JADa8u93M3ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see below for standard context options, for gpt-3, the reduced context sets have the following names:\n",
        "# processed_website_text -> processed_website_text_longexcluded\n",
        "# processed_schedule_tables -> processed_schedule_tables_textified_small\n",
        "contexts = [[\"processed_website_tables\",\"processed_website_text\",\"processed_schedule_tables\"], \n",
        "            [\"processed_website_tables\",\"processed_website_text\"],\n",
        "            [\"processed_website_tables\"],\n",
        "            [\"processed_website_text\"],\n",
        "            [\"processed_schedule_tables\"]]\n",
        "\n",
        "text_readers = [\"minilm\", \"distilroberta\", \"electra-base\", \"deberta-large\", \"bert-base\", \"gpt3\"]\n",
        "table_reader = \"tapas\"\n",
        "seperate_evaluation = True\n",
        "api_key = \"<insert api_key>\"\n",
        "top_ks = [1,3,10]\n",
        "use_ada_embeddings = False\n"
      ],
      "metadata": {
        "id": "StHg2-0rpfRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./mnt/MyDrive/Erasmus/Transformer_Pretraining/Bounwiki/"
      ],
      "metadata": {
        "id": "kulebLufNDXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from run import main"
      ],
      "metadata": {
        "id": "NhWYD7Ljvk3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example run\n",
        "main(contexts[0], text_readers[0], table_reader, seperate_evaluation, api_key, top_ks[0], use_ada_embeddings)"
      ],
      "metadata": {
        "id": "1OXIYzg-vpET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Look at Results**"
      ],
      "metadata": {
        "id": "K3A9bAXt54LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example to read out results, first load \n",
        "import pandas as pd\n",
        "df = pd.read_csv('./output/results.csv')"
      ],
      "metadata": {
        "id": "P4A68HTG1729"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter results as wished, round the numbers and save to seperate csv\n",
        "df[(df[\"Topk\"]==3.0) & (df[\"Label type\"]==\"all_eval\") & (df[\"Context\"]==\"processed_website_tables_processed_website_text_processed_schedule_tables\")].round(3).to_csv(\"./output/all_eval_top_3__all_data.csv\")"
      ],
      "metadata": {
        "id": "XoxrY5Oj2FJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run.py --context \"processed_website_text\" --text_reader \"bert-base\" --table_reader \"tapas\" --seperate_evaluation"
      ],
      "metadata": {
        "id": "K8C8aOWQM50g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**More Ideas**"
      ],
      "metadata": {
        "id": "_ImJKyeI6TqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install tabulate to convert tables to text style tables and remove the need for table reader\n",
        "!pip install tabulate"
      ],
      "metadata": {
        "id": "tBR6-MfN-xEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "import pickle\n",
        "# load the schedule tables\n",
        "with open(\"/content/mnt/MyDrive/Erasmus/Transformer_Pretraining/Bounwiki/data/website_data/processed_schedule_tables\", \"rb\") as fp:\n",
        "  res = pickle.load(fp)\n",
        "# formate them to table, and save the result if desired\n",
        "print(tabulate(res[0].content, headers = 'keys', tablefmt = 'psql'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D44z1VRW-0Hl",
        "outputId": "7b4613eb-6ed3-494d-9d04-cd922cf158ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------+------------------------------------------+--------+--------------------------+------------------+--------------------+-------------------+\n",
            "|    | Code       | Name                                     |   Ects | Instructor               | Days             | Hours              | Rooms             |\n",
            "|----+------------+------------------------------------------+--------+--------------------------+------------------+--------------------+-------------------|\n",
            "|  0 | ASIA502.02 | READINGS ON ASIAN ECONOMIES              |      8 | ALTAY ATLI               | MMM              | 101112             | No Room specified |\n",
            "|  1 | ASIA518.02 | HISTORY OF MODERN JAPAN                  |      8 | SELÇUK ESENBEL           | TTT              | 101112             | No Room specified |\n",
            "|  2 | ASIA520.02 | JAPANESE SOCIETY THROUGH MODERN LITERATU |      8 | OĞUZ BAYKARA             | TTT              | 101112             | No Room specified |\n",
            "|  3 | ASIA521.02 | JAPANESE SOCIETY THROUGH ART,&CULTURE    |      8 | ERDAL KÜÇÜKYALÇIN        | ThThTh           | 101112             | No Room specified |\n",
            "|  4 | ASIA526.02 | AN INTRODUCTION TO CHINESE HISTORY       |      8 | ZEYNEP HALE EROĞLU SAGER | WWW              | 101112             | No Room specified |\n",
            "|  5 | ASIA528.02 | CHINA'S INTER.REL.&FOREIGN POLICY        |      8 | ZEYNEP HALE EROĞLU SAGER | MMM              | 91011              | No Room specified |\n",
            "|  6 | ASIA538.02 | RUSSIA IN ASIA                           |      8 | ZEYNEP HALE EROĞLU SAGER | WWW              | 91011              | No Room specified |\n",
            "|  7 | ASIA549.01 | PROJECT PAPER                            |     14 | ZEYNEP HALE EROĞLU SAGER | No Day specified | No Hours specified | No Room specified |\n",
            "|  8 | ASIA579.01 | GRADUATE SEMINAR                         |      2 | ARZU ÖZTÜRKMEN           | No Day specified | No Hours specified | No Room specified |\n",
            "|  9 | ASIL515.01 | BEGINNERS KOREAN I                       |      6 | HWA CHEON H.LEE          | StStSt           | 345                | No Room specified |\n",
            "+----+------------+------------------------------------------+--------+--------------------------+------------------+--------------------+-------------------+\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "a34LjFJP0gku",
        "9nW6aMGPxn-X",
        "EnmpFC1qxn-d",
        "K3A9bAXt54LU",
        "_ImJKyeI6TqK"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "999ea782e2d719ec62688e738a2ff20f2535cd73f1388dd13a2d835295a4fc1a"
      }
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}