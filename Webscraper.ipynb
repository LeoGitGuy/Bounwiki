{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For using COLAB (recommended)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd ./drive/MyDrive/Erasmus/Transformer_Pretraining/Bounwiki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install all if not using colab\n",
    "#!pip install -r requirements.txt\n",
    "# Haystack is the only not preinstalled package for this notebook\n",
    "# RESTART KERNEL after install\n",
    "#!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import pandas as pd\n",
    "#from haystack import Document\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TABULAR COURSE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. go to boun website and get a list of all courses and create a file \n",
    "# courses_list.txt with it\n",
    "\n",
    "# 2. get course list and parse\n",
    "with open('./data/website_data/courses_list.txt', 'r') as my_file:\n",
    "  courses_list = my_file.read()\n",
    "courses = pd.read_html(courses_list, flavor=\"lxml\")[0]\n",
    "courses = courses.rename({courses.columns[0]: 'Name'}, axis=1)\n",
    "list_soup = soup(courses_list, \"html.parser\")\n",
    "list_table = list_soup.find_all('table')[0]\n",
    "links = []\n",
    "for tr in list_table.find_all(\"tr\"):\n",
    "    trs = tr.find_all(\"td\")\n",
    "    for each in trs:\n",
    "        try:\n",
    "            link = each.find('a')['href']\n",
    "            links.append(link)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "courses[\"Link\"] = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download schedules and filter [function]\n",
    "import re\n",
    "def extract_tables(courses):\n",
    "  courses_contents = []\n",
    "  processed_tables = []\n",
    "  for index, row in courses.iterrows():\n",
    "    page = requests.get(\"https://registration.boun.edu.tr/\" + row[\"Link\"])\n",
    "    html_table = soup(page.content, \"html.parser\").find_all('table')[2]\n",
    "    course_contents = pd.read_html(str(html_table), flavor=\"lxml\")[0]\n",
    "    course_contents = course_contents.rename(columns=course_contents.iloc[0]).drop(course_contents.index[0]).reset_index(drop=True)\n",
    "    course_contents.drop(['Desc.', 'Cr.', 'Quota', 'Course Delivery Method', 'Exam', 'Sl.', 'Required for Dept.(*)', 'Departments'], axis=1, inplace = True)\n",
    "    course_contents.rename({'Code.Sec': 'Code', 'Instr.': 'Instructor'}, axis=1, inplace = True)\n",
    "    course_contents.style.set_caption(row[\"Name\"])\n",
    "    course_contents[\"Days\"] = course_contents[\"Days\"].fillna(\"No Day specified\")\n",
    "    course_contents[\"Hours\"] = course_contents[\"Hours\"].fillna(\"No Hours specified\")\n",
    "    course_contents[\"Rooms\"] = course_contents[\"Rooms\"].fillna(\"No Room specified\")\n",
    "    course_contents[\"Instructor\"] = course_contents[\"Instructor\"].fillna(\"No Instructor specified\")\n",
    "    courses_contents[\"Code\"] = course_contents[\"Code\"].fillna(\"No Code specified\")\n",
    "    courses_contents[\"Name\"] = course_contents[\"Name\"].fillna(\"No Name specified\")\n",
    "    courses_contents[\"Ects\"] = course_contents[\"Ects\"].fillna(\"No Ects specified\")\n",
    "    \n",
    "    courses_contents.append(course_contents)\n",
    "    faculty = course_contents.iloc[0,0]\n",
    "    faculty_abb = re.search(r\"[a-z]*\", faculty, re.IGNORECASE).group()\n",
    "    document = Document(content=course_contents, content_type=\"table\", id=faculty_abb)\n",
    "    processed_tables.append(document)\n",
    "  return courses_contents, processed_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download schedules and filter\n",
    "courses_contents, processed_tables = extract_tables(courses)\n",
    "# with open(\"./data/website_data/processed_schedule_tables\", \"wb\") as fp:\n",
    "#     pickle.dump(processed_tables, fp)\n",
    "    \n",
    "# use courses_contents for visualization, e.g.\n",
    "# courses_contents[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Website Information Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_info = pd.read_excel('./data/website_data/urls_website.xlsx', engine=\"openpyxl\")\n",
    "def extract_texts(website_info):\n",
    "  table_dfs = []\n",
    "  processed_text_content = []\n",
    "  processed_table_content = []\n",
    "  website_df = pd.DataFrame(columns = ['document_text', 'document_identifier'])\n",
    "  for index, row in website_info.iterrows():\n",
    "    if row[\"Kind\"] == \"text\" and \"intl.boun.edu.tr/\" not in row[\"Url\"]:\n",
    "      page = requests.get(row[\"Url\"])\n",
    "      s = soup(page.content, \"html.parser\")\n",
    "      text_passages = [x.getText().replace('\\xa0', ' ') for x in s.find(class_='content').find_all('p')]\n",
    "      text_content = (\"\\n\").join(text_passages)\n",
    "      website_df = website_df.append({'document_text' : text_content, 'document_identifier' : row[\"Topic\"]},  ignore_index = True)\n",
    "      #processed_text_content.append(Document(content=text_content, content_type=\"text\", id=row[\"Topic\"]))\n",
    "    elif row[\"Kind\"] == \"table\" or \"intl.boun.edu.tr/\" in row[\"Url\"]:\n",
    "      page = requests.get(row[\"Url\"])\n",
    "      text_tables = []\n",
    "      if \"intl.boun.edu.tr/\" in row[\"Url\"]:\n",
    "        html_tables = soup(page.content, \"html.parser\").find(class_='region-content').find_all('table')\n",
    "      elif \"boun.edu.tr/\" in row[\"Url\"]:\n",
    "        html_tables = soup(page.content, \"html.parser\").find(class_='content').find_all('table')\n",
    "      #print(f\"# tables: {len(html_tables)}\")\n",
    "      for html_table in html_tables:\n",
    "        if len(html_table.findAll(lambda tag: tag.name == 'tr')) == 1:\n",
    "          text_passage = html_table.getText().replace('\\xa0', ' ')\n",
    "          text_tables.append(text_passage)\n",
    "\n",
    "        elif len(html_table.findAll(lambda tag: tag.name == 'tr')) > 1: \n",
    "          table_df = pd.read_html(str(html_table))[0]\n",
    "          table_df = table_df.rename(columns=table_df.iloc[0]).drop(table_df.index[0]).reset_index(drop=True)\n",
    "          table_dfs.append(table_df)\n",
    "          \n",
    "          #document = Document(content=table_df, content_type=\"table\", id=index)\n",
    "          #processed_table_content.append(document)\n",
    "      if len(text_tables) > 0:\n",
    "        text_content = (\"\\n\").join(text_tables)\n",
    "        website_df = website_df.append({'document_text' : text_content, 'document_identifier' : row[\"Topic\"]},  ignore_index = True)\n",
    "        processed_text_content.append(Document(content=text_content, content_type=\"text\", id=row[\"Topic\"]))\n",
    "        \n",
    "  return website_df, table_dfs, processed_text_content, processed_table_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "lxml not found, please install it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5096\\2260728800.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_dfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocessed_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocessed_tables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwebsite_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# with open(\"./data/website_data/processed_website_text\", \"wb\") as fp:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#     pickle.dump(processed_text, fp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# with open(\"./data/website_data/processed_website_tables\", \"wb\") as fp:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#     pickle.dump(processed_tables, fp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5096\\3357650483.py\u001b[0m in \u001b[0;36mextract_texts\u001b[1;34m(website_info)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m           \u001b[0mtable_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m           \u001b[0mtable_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtable_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m           \u001b[0mtable_dfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m                 )\n\u001b[0;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\html.py\u001b[0m in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only)\u001b[0m\n\u001b[0;32m   1099\u001b[0m         \u001b[0mna_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m         \u001b[0mkeep_default_na\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_default_na\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1101\u001b[1;33m         \u001b[0mdisplayed_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisplayed_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1102\u001b[0m     )\n",
      "\u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\html.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, **kwargs)\u001b[0m\n\u001b[0;32m    892\u001b[0m     \u001b[0mretained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mflav\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mflavor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 894\u001b[1;33m         \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflav\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    895\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompiled_match\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplayed_only\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\html.py\u001b[0m in \u001b[0;36m_parser_dispatch\u001b[1;34m(flavor)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_HAS_LXML\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 851\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lxml not found, please install it\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    852\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_valid_parsers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mflavor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: lxml not found, please install it"
     ]
    }
   ],
   "source": [
    "w_df, t_dfs, processed_text, processed_tables = extract_texts(website_info)\n",
    "\n",
    "# with open(\"./data/website_data/processed_website_text\", \"wb\") as fp:\n",
    "#     pickle.dump(processed_text, fp)\n",
    "# with open(\"./data/website_data/processed_website_tables\", \"wb\") as fp:\n",
    "#     pickle.dump(processed_tables, fp)\n",
    "for idx, table in enumerate(t_dfs):\n",
    "    table.to_csv(f\"{idx}.txt\", index=False)\n",
    "# use w_df and t_dfs for visualization, e.g.\n",
    "# w_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "999ea782e2d719ec62688e738a2ff20f2535cd73f1388dd13a2d835295a4fc1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
