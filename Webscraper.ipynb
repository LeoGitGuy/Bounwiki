{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from haystack import Document\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TABULAR COURSE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. go to boun website and get a list of all courses and create a file \n",
    "# courses_list.txt with it\n",
    "\n",
    "# 2. get course list and parse\n",
    "with open('./data/website_data/courses_list.txt', 'r') as my_file:\n",
    "  courses_list = my_file.read()\n",
    "courses = pd.read_html(courses_list, flavor=\"lxml\")[0]\n",
    "courses = courses.rename({courses.columns[0]: 'Name'}, axis=1)\n",
    "list_soup = soup(courses_list, \"html.parser\")\n",
    "list_table = list_soup.find_all('table')[0]\n",
    "links = []\n",
    "for tr in list_table.find_all(\"tr\"):\n",
    "    trs = tr.find_all(\"td\")\n",
    "    for each in trs:\n",
    "        try:\n",
    "            link = each.find('a')['href']\n",
    "            links.append(link)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "courses[\"Link\"] = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download schedules and filter [function]\n",
    "def extract_tables(courses):\n",
    "  courses_contents = []\n",
    "  processed_tables = []\n",
    "  for index, row in courses[3:5].iterrows():\n",
    "    page = requests.get(\"https://registration.boun.edu.tr/\" + row[\"Link\"])\n",
    "    html_table = soup(page.content, \"html.parser\").find_all('table')[2]\n",
    "    course_contents = pd.read_html(str(html_table), flavor=\"lxml\")[0]\n",
    "    course_contents = course_contents.rename(columns=course_contents.iloc[0]).drop(course_contents.index[0]).reset_index(drop=True)\n",
    "    course_contents.drop(['Desc.', 'Cr.', 'Quota', 'Course Delivery Method', 'Exam', 'Sl.', 'Required for Dept.(*)', 'Departments'], axis=1, inplace = True)\n",
    "    course_contents.rename({'Code.Sec': 'Code', 'Instr.': 'Instructor'}, axis=1, inplace = True)\n",
    "    course_contents.style.set_caption(row[\"Name\"])\n",
    "    course_contents[\"Days\"] = course_contents[\"Days\"].fillna(\"No Day specified\")\n",
    "    course_contents[\"Hours\"] = course_contents[\"Hours\"].fillna(\"No Hours specified\")\n",
    "    course_contents[\"Rooms\"] = course_contents[\"Rooms\"].fillna(\"No Room specified\")\n",
    "    course_contents[\"Instructor\"] = course_contents[\"Instructor\"].fillna(\"No Instructor specified\")\n",
    "\n",
    "    courses_contents.append(course_contents)\n",
    "    document = Document(content=course_contents, content_type=\"table\", id=index)\n",
    "    processed_tables.append(document)\n",
    "  return courses_contents, processed_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download schedules and filter\n",
    "courses_contents, processed_tables = extract_tables(courses)\n",
    "with open(\"./data/website_data/processed_schedule_tables\", \"wb\") as fp:\n",
    "    pickle.dump(processed_tables, fp)\n",
    "    \n",
    "# use courses_contents for visualization, e.g.\n",
    "# courses_contents[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Website Information Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_info = pd.read_excel('./urls_website.xlsx')\n",
    "def extract_texts(website_info):\n",
    "  table_dfs = []\n",
    "  processed_text_content = []\n",
    "  processed_table_content = []\n",
    "  website_df = pd.DataFrame(columns = ['Name', 'Content'])\n",
    "  for index, row in website_info[:13].iterrows():\n",
    "    if row[\"Kind\"] == \"text\" and \"intl.boun.edu.tr/\" not in row[\"Url\"]:\n",
    "      page = requests.get(row[\"Url\"])\n",
    "      s = soup(page.content, \"html.parser\")\n",
    "      text_passages = [x.getText().replace('\\xa0', ' ') for x in s.find(class_='content').find_all('p')]\n",
    "      text_content = (\"\\n\").join(text_passages)\n",
    "      website_df = website_df.append({'Name' : row[\"Topic\"], 'Content' : text_content}, ignore_index = True)\n",
    "      processed_text_content.append(Document(content=text_content, content_type=\"text\", id=row[\"Topic\"]))\n",
    "    elif row[\"Kind\"] == \"table\" or \"intl.boun.edu.tr/\" in row[\"Url\"]:\n",
    "      page = requests.get(row[\"Url\"])\n",
    "      text_tables = []\n",
    "      if \"intl.boun.edu.tr/\" in row[\"Url\"]:\n",
    "        html_tables = soup(page.content, \"html.parser\").find(class_='region-content').find_all('table')\n",
    "      elif \"boun.edu.tr/\" in row[\"Url\"]:\n",
    "        html_tables = soup(page.content, \"html.parser\").find(class_='content').find_all('table')\n",
    "      #print(f\"# tables: {len(html_tables)}\")\n",
    "      for html_table in html_tables:\n",
    "        if len(html_table.findAll(lambda tag: tag.name == 'tr')) == 1:\n",
    "          text_passage = html_table.getText().replace('\\xa0', ' ')\n",
    "          text_tables.append(text_passage)\n",
    "\n",
    "        elif len(html_table.findAll(lambda tag: tag.name == 'tr')) > 1: \n",
    "          table_df = pd.read_html(str(html_table), flavor=\"lxml\")[0]\n",
    "          table_df = table_df.rename(columns=table_df.iloc[0]).drop(table_df.index[0]).reset_index(drop=True)\n",
    "          table_dfs.append(table_df)\n",
    "          document = Document(content=table_df, content_type=\"table\", id=index)\n",
    "          processed_table_content.append(document)\n",
    "      if len(text_tables) > 0:\n",
    "        text_content = (\"\\n\").join(text_tables)\n",
    "        website_df.append({'Name' : row[\"Topic\"], 'Content' : (\"\\n\").join(text_tables)}, ignore_index = True)\n",
    "        processed_text_content.append(Document(content=text_content, content_type=\"text\", id=row[\"Topic\"]))\n",
    "        \n",
    "  return website_df, table_dfs, processed_text_content, processed_table_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_df, t_dfs, processed_text, processed_tables = extract_texts(website_info)\n",
    "with open(\"./data/website_data/processed_website_text\", \"wb\") as fp:\n",
    "    pickle.dump(processed_text, fp)\n",
    "with open(\"./data/website_data/processed_website_tables\", \"wb\") as fp:\n",
    "    pickle.dump(processed_tables, fp)\n",
    "    \n",
    "# use w_df and t_dfs for visualization, e.g.\n",
    "# w_df.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
