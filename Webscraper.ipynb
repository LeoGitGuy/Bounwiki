{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For using COLAB (recommended)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd .\"<path>\"/Bounwiki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install all if not using colab\n",
    "#!pip install -r requirements.txt\n",
    "#Haystack is the only not preinstalled package for this notebook, for install details see\n",
    "https://github.com/deepset-ai/haystack#floppy_disk-installation\n",
    "#RESTART KERNEL after install\n",
    "!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from haystack import Document\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TABULAR COURSE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. go to boun website and get a list of all courses and create a file \n",
    "# courses_list.txt with it\n",
    "\n",
    "# 2. get course list and parse\n",
    "with open('./data/website_data/courses_list.txt', 'r') as my_file:\n",
    "  courses_list = my_file.read()\n",
    "courses = pd.read_html(courses_list, flavor=\"lxml\")[0]\n",
    "courses = courses.rename({courses.columns[0]: 'Name'}, axis=1)\n",
    "list_soup = soup(courses_list, \"html.parser\")\n",
    "list_table = list_soup.find_all('table')[0]\n",
    "links = []\n",
    "for tr in list_table.find_all(\"tr\"):\n",
    "    trs = tr.find_all(\"td\")\n",
    "    for each in trs:\n",
    "        try:\n",
    "            link = each.find('a')['href']\n",
    "            links.append(link)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "courses[\"Link\"] = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download schedules and filter [function]\n",
    "import re\n",
    "def extract_tables(courses):\n",
    "  courses_contents = []\n",
    "  processed_tables = []\n",
    "  for index, row in courses.iterrows():\n",
    "    page = requests.get(\"https://registration.boun.edu.tr/\" + row[\"Link\"])\n",
    "    html_table = soup(page.content, \"html.parser\").find_all('table')[2]\n",
    "    course_contents = pd.read_html(str(html_table), flavor=\"lxml\")[0]\n",
    "    course_contents = course_contents.rename(columns=course_contents.iloc[0]).drop(course_contents.index[0]).reset_index(drop=True)\n",
    "    course_contents.drop(['Desc.', 'Cr.', 'Quota', 'Course Delivery Method', 'Exam', 'Sl.', 'Required for Dept.(*)', 'Departments'], axis=1, inplace = True)\n",
    "    course_contents.rename({'Code.Sec': 'Code', 'Instr.': 'Instructor'}, axis=1, inplace = True)\n",
    "    course_contents.style.set_caption(row[\"Name\"])\n",
    "    course_contents[\"Days\"] = course_contents[\"Days\"].fillna(\"No Day specified\")\n",
    "    course_contents[\"Hours\"] = course_contents[\"Hours\"].fillna(\"No Hours specified\")\n",
    "    course_contents[\"Rooms\"] = course_contents[\"Rooms\"].fillna(\"No Room specified\")\n",
    "    course_contents[\"Instructor\"] = course_contents[\"Instructor\"].fillna(\"No Instructor specified\")\n",
    "    courses_contents[\"Code\"] = course_contents[\"Code\"].fillna(\"No Code specified\")\n",
    "    courses_contents[\"Name\"] = course_contents[\"Name\"].fillna(\"No Name specified\")\n",
    "    courses_contents[\"Ects\"] = course_contents[\"Ects\"].fillna(\"No Ects specified\")\n",
    "    \n",
    "    courses_contents.append(course_contents)\n",
    "    faculty = course_contents.iloc[0,0]\n",
    "    faculty_abb = re.search(r\"[a-z]*\", faculty, re.IGNORECASE).group()\n",
    "    document = Document(content=course_contents, content_type=\"table\", id=faculty_abb)\n",
    "    processed_tables.append(document)\n",
    "  return courses_contents, processed_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download schedules and filter\n",
    "courses_contents, processed_tables = extract_tables(courses)\n",
    "with open(\"./data/website_data/processed_schedule_tables\", \"wb\") as fp:\n",
    "    pickle.dump(processed_tables, fp)\n",
    "    \n",
    "# Example:\n",
    "# courses_contents[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Website Information Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if wanted, extend './data/website_data/urls_website.xlsx' file with more pages\n",
    "website_info = pd.read_excel('./data/website_data/urls_website.xlsx', engine=\"openpyxl\")\n",
    "def extract_texts(website_info):\n",
    "  table_dfs = []\n",
    "  processed_text_content = []\n",
    "  processed_table_content = []\n",
    "  website_df = pd.DataFrame(columns = ['document_text', 'document_identifier'])\n",
    "  website_key = \"1173318\"\n",
    "  for index, row in website_info.iterrows():\n",
    "    # if content on website is text and it is from the regular boun page\n",
    "    if row[\"Kind\"] == \"text\" and \"intl.boun.edu.tr/\" not in row[\"Url\"]:\n",
    "      page = requests.get(row[\"Url\"])\n",
    "      s = soup(page.content, \"html.parser\")\n",
    "      text_passages = [x.getText().replace('\\xa0', ' ') for x in s.find(class_='content').find_all('p')]\n",
    "      text_content = (\"\\n\").join(text_passages)\n",
    "      website_df = website_df.append({'document_text' : text_content, 'document_identifier' : row[\"Topic\"]},  ignore_index = True)\n",
    "      processed_text_content.append(Document(content=text_content, content_type=\"text\", id=website_key))\n",
    "      website_key = str(int(website_key) + 1)\n",
    "    # if content on website is from kind table or it is from homepage of international office (different web structure)\n",
    "    elif row[\"Kind\"] == \"table\" or \"intl.boun.edu.tr/\" in row[\"Url\"]:\n",
    "      page = requests.get(row[\"Url\"])\n",
    "      text_tables = []\n",
    "      # if content if from international office\n",
    "      if \"intl.boun.edu.tr/\" in row[\"Url\"]:\n",
    "        html_tables = soup(page.content, \"html.parser\").find(class_='region-content').find_all('table')\n",
    "      # if content is from regular boun page\n",
    "      elif \"boun.edu.tr/\" in row[\"Url\"]:\n",
    "        html_tables = soup(page.content, \"html.parser\").find(class_='content').find_all('table')\n",
    "      # go through all tables found on page and extract content\n",
    "      for html_table in html_tables:\n",
    "        # if there is only one row, assume that this is just text but formatted as table)\n",
    "        if len(html_table.findAll(lambda tag: tag.name == 'tr')) == 1:\n",
    "          text_passage = html_table.getText().replace('\\xa0', ' ')\n",
    "          text_tables.append(text_passage)\n",
    "        # if there are more rows, treat it as table\n",
    "        elif len(html_table.findAll(lambda tag: tag.name == 'tr')) > 1: \n",
    "          table_df = pd.read_html(str(html_table))[0]\n",
    "          table_df = table_df.rename(columns=table_df.iloc[0]).drop(table_df.index[0]).reset_index(drop=True)\n",
    "          table_dfs.append(table_df)\n",
    "          document = Document(content=table_df, content_type=\"table\", id=index)\n",
    "          processed_table_content.append(document)\n",
    "      if len(text_tables) > 0:\n",
    "        text_content = (\"\\n\").join(text_tables)\n",
    "        website_df = website_df.append({'document_text' : text_content, 'document_identifier' : row[\"Topic\"]},  ignore_index = True)\n",
    "        processed_text_content.append(Document(content=text_content, content_type=\"text\", id=website_key))\n",
    "        website_key = str(int(website_key) + 1)\n",
    "        \n",
    "  return website_df, table_dfs, processed_text_content, processed_table_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_df, t_dfs, processed_text, processed_tables = extract_texts(website_info)\n",
    "# save website text\n",
    "with open(\"./data/website_data/processed_website_text\", \"wb\") as fp:\n",
    "    pickle.dump(processed_text, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# convert tables to text\n",
    "for idx, table in enumerate(t_dfs):\n",
    "    table.to_csv(f\"./data/website_data/web_tables_txt/{idx}.txt\", index=False, sep=\"\\t\")\n",
    "# only keep relevant tables, this should be done manually by looking which files are useful information\n",
    "directory = \"./data/website_data/web_tables_txt/\"\n",
    "processed_website_tables = []\n",
    "keep_indices = [1,5,7,8,10,14,15]\n",
    "table_ids = [80,72,73,74,75,76,77]\n",
    "filecount = 0\n",
    "key_prefix = \"12025\"\n",
    "for filename in os.listdir(directory):\n",
    "    if int(filename.split(\".\")[0]) in keep_indices:\n",
    "        f = os.path.join(directory, filename)\n",
    "        key = key_prefix + str(table_ids[filecount])\n",
    "        # checking if it is a file\n",
    "        with open(f,\"r\") as file:\n",
    "            content = file.read()\n",
    "            document = Document(content=content, content_type=\"text\", id=key)\n",
    "            processed_website_tables.append(document)\n",
    "            filecount += 1\n",
    "# save website tables\n",
    "with open(\"./data/website_data/processed_website_tables\", \"wb\") as fp:\n",
    "    pickle.dump(processed_website_tables, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "999ea782e2d719ec62688e738a2ff20f2535cd73f1388dd13a2d835295a4fc1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
